import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { GoogleGenerativeAI } from '@google/generative-ai';

export interface AIAnalysis {
  sentiment: 'positive' | 'neutral' | 'negative';
  urgency: 'low' | 'normal' | 'high' | 'urgent';
  intent: 'question' | 'complaint' | 'purchase' | 'support' | 'greeting' | 'other';
  confidence: number;
  reasoning: string;
  shouldEscalate: boolean;
  escalationReason?: string;
}

@Injectable()
export class AIAnalysisService {
  private readonly logger = new Logger(AIAnalysisService.name);
  private genAI: GoogleGenerativeAI;
  private readonly MODEL_NAME: string;

  constructor(private readonly config: ConfigService) {
    const apiKey = this.config.get('GEMINI_API_KEY');
    this.genAI = new GoogleGenerativeAI(apiKey || '');
    this.MODEL_NAME = this.config.get('GEMINI_MODEL') || 'gemini-2.0-flash';
  }

  /**
   * Analisa mensagem para extrair inten√ß√£o, sentimento e urg√™ncia
   */
  async analyzeMessage(
    messageContent: string,
    context: any,
  ): Promise<AIAnalysis> {
    try {
      const model = this.genAI.getGenerativeModel({ model: this.MODEL_NAME });

      const prompt = `Analise a seguinte mensagem recebida no WhatsApp de uma empresa.
      
      MENSAGEM: "${messageContent}"
      
      CONTEXTO:
      - Hist√≥rico recente: ${JSON.stringify(context.conversationHistory?.slice(-3) || [])}
      - Cliente: ${context.contactName || 'Desconhecido'}

      Responda APENAS um JSON com o seguinte formato:
      {
        "sentiment": "positive" | "neutral" | "negative",
        "urgency": "low" | "normal" | "high" | "urgent",
        "intent": "question" | "complaint" | "purchase" | "support" | "greeting" | "other",
        "confidence": 0.0 a 1.0,
        "reasoning": "explica√ß√£o breve",
        "shouldEscalate": boolean (true se precisar de humano urgente),
        "escalationReason": "motivo para chamar humano (opcional)"
      }
      
      REGRAS DE ESCALONAMENTO:
      - Clientes muito irritados ou xingando -> shouldEscalate: true
      - Pedidos expl√≠citos para falar com atendente -> shouldEscalate: true
      - Assuntos complexos que a IA n√£o sabe resolver -> shouldEscalate: true
      `;

      const result = await model.generateContent(prompt);
      const responseText = result.response.text();

      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        // Fallback seguro
        return {
          sentiment: 'neutral',
          urgency: 'normal',
          intent: 'other',
          confidence: 0,
          reasoning: 'Falha ao analisar JSON',
          shouldEscalate: false
        };
      }

      return JSON.parse(jsonMatch[0]);

    } catch (error) {
      this.logger.error(`Error analyzing message: ${error.message}`);
      // Fallback em caso de erro
      return {
        sentiment: 'neutral',
        urgency: 'normal',
        intent: 'other',
        confidence: 0,
        reasoning: 'Erro na API de an√°lise',
        shouldEscalate: false
      };
    }
  }

  /**
   * Qualifica leads com base no hist√≥rico
   */
  async qualifyLead(params: { contactId: string; messages: any[]; contact: any; memoryContext?: string }): Promise<any> {
    try {
      const { contactId, messages, contact, memoryContext } = params;
      const model = this.genAI.getGenerativeModel({ model: this.MODEL_NAME });

      const conversationSummary = messages
        .slice(-20) // √öltimas 20 msgs
        .map((m) => `[${m.direction === 'incoming' ? 'CONTATO' : 'EU'}]: ${m.content}`)
        .join('\n');

      const prompt = `ATUE COMO UM ESPECIALISTA EM VENDAS E QUALIFICA√á√ÉO DE LEADS.
      
CONTEXTO DO CONTATO:
Nome: ${contact.pushName}
Telefone: ${contact.remoteJid}
Mem√≥rias Importantes:
${memoryContext || 'Nenhuma mem√≥ria salva ainda'}

HIST√ìRICO DA CONVERSA (${messages.length} mensagens):
- [EU] = minhas mensagens (ignore para a an√°lise do perfil)
- [CONTATO] = mensagens do contato (FOCO DA AN√ÅLISE)

${conversationSummary}

FA√áA UMA AN√ÅLISE PROFUNDA SOBRE O CONTATO "${contact.pushName || 'CONTATO'}" E RETORNE JSON:
{
  "score": 0-100,
  "status": "cold|warm|hot|qualified|customer",
  "analysis": "Escreva uma an√°lise DETALHADA de 4-5 par√°grafos sobre o CONTATO cobrindo perfil, interesses, obje√ß√µes e recomenda√ß√µes."
}`;

      const result = await model.generateContent(prompt);
      const response = result.response.text();
      const jsonMatch = response.match(/\{[\s\S]*\}/);

      if (!jsonMatch) throw new Error('Could not parse lead analysis JSON');
      return JSON.parse(jsonMatch[0]);

    } catch (error) {
      this.logger.error(`Lead qualification failed: ${error.message}`);
      throw error;
    }
  }

  async summarizeConversation(messages: any[]): Promise<string> {
    const model = this.genAI.getGenerativeModel({ model: this.MODEL_NAME });
    const conversationText = messages.map(m => `${m.direction === 'incoming' ? 'Cliente' : 'Voc√™'}: ${m.content}`).join('\n');
    const result = await model.generateContent(`Resuma a seguinte conversa em 2-3 frases:\n${conversationText}`);
    return result.response.text().trim();
  }

  async detectOpportunity(messageContent: string, products: any[]): Promise<any> {
    const model = this.genAI.getGenerativeModel({ model: this.MODEL_NAME });
    const productsList = products.map(p => `- ${p.name}: ${p.price}`).join('\n');
    const result = await model.generateContent(`Analise se h√° oportunidade de venda na mensagem: "${messageContent}"\nProdutos: ${productsList}\nRetorne JSON: { "hasOpportunity": bool, "recommendedProducts": [], "reasoning": "" }`);
    const jsonMatch = result.response.text().match(/\{[\s\S]*\}/);
    return jsonMatch ? JSON.parse(jsonMatch[0]) : { hasOpportunity: false };
  }

  /**
   * OTIMIZADO: Analisa E gera resposta em uma √∫nica chamada ao Gemini
   * Economiza 50% das chamadas de API em modo ativo
   * MELHORADO: Usa mais contexto de conversa para manter coer√™ncia
   */
  async analyzeAndRespond(
    messageContent: string,
    context: any,
    aiConfig: any,
    enrichedContext?: {
      relevantKnowledge?: string;
      memoryContext?: string;
      pendingItems?: string;
    },
  ): Promise<{
    analysis: AIAnalysis;
    response: string;
  }> {
    try {
      const model = this.genAI.getGenerativeModel({
        model: this.MODEL_NAME,
        generationConfig: {
          temperature: aiConfig.temperature || 0.7,
        }
      });

      const ownerName = aiConfig.ownerName || 'o respons√°vel';

      // Contexto de produtos
      const productContext = context.products?.length > 0
        ? `PRODUTOS DISPON√çVEIS:\n${context.products.map((p: any) => `- ${p.name}: R$${p.price}${p.quantity > 0 ? '' : ' (esgotado)'}`).join('\n')}`
        : '';

      // Formatar hist√≥rico de conversa de forma mais detalhada (mais mensagens)
      const conversationHistory = this.formatConversationHistory(context.conversationHistory, 15);

      // Montar se√ß√µes de contexto enriquecido
      let extraContext = '';
      if (enrichedContext?.relevantKnowledge) {
        extraContext += `\n${enrichedContext.relevantKnowledge}\n`;
      }
      if (enrichedContext?.memoryContext) {
        extraContext += `\n${enrichedContext.memoryContext}\n`;
      }
      if (enrichedContext?.pendingItems) {
        extraContext += `\n${enrichedContext.pendingItems}\n`;
      }

      const prompt = `Voc√™ √© Sofia, secret√°ria virtual simp√°tica do ${ownerName}. Analise a mensagem e responda.

IMPORTANTE: Leia ATENTAMENTE todo o hist√≥rico da conversa antes de responder. Mantenha coer√™ncia com o que j√° foi discutido.
Se o cliente est√° respondendo a uma pergunta sua, USE essa resposta no seu racioc√≠nio.
NUNCA "esque√ßa" o que j√° foi conversado. Mantenha o fluxo natural da conversa.

---

MENSAGEM DO CLIENTE AGORA: "${messageContent}"
NOME DO CLIENTE: ${context.contactName || 'Cliente'}

HIST√ìRICO DA CONVERSA (do mais antigo ao mais recente):
${conversationHistory || 'In√≠cio da conversa'}

${productContext}

${extraContext}

${aiConfig.systemPrompt || ''}

---

ANALISE O CONTEXTO COMPLETO e responda em JSON:
{
  "analysis": {
    "sentiment": "positive" | "neutral" | "negative",
    "urgency": "low" | "normal" | "high" | "urgent",
    "intent": "question" | "complaint" | "purchase" | "support" | "greeting" | "other" | "followup",
    "confidence": 0.0 a 1.0,
    "reasoning": "explica√ß√£o de COMO voc√™ interpretou a mensagem no contexto da conversa",
    "shouldEscalate": boolean (true se precisar chamar ${ownerName}),
    "escalationReason": "motivo (se shouldEscalate true)"
  },
  "response": "Sua resposta natural e COERENTE com o hist√≥rico da conversa"
}

REGRAS CR√çTICAS:
1. Se o cliente est√° respondendo a algo que voc√™ perguntou ‚Üí Use essa resposta
2. Se j√° discutiram um assunto ‚Üí Continue nesse assunto naturalmente
3. Nunca repita perguntas que o cliente j√° respondeu
4. Se o cliente pedir para falar com humano ‚Üí shouldEscalate: true
5. Se reclama√ß√£o s√©ria ou cliente irritado ‚Üí shouldEscalate: true
6. Se n√£o souber responder ‚Üí shouldEscalate: true
7. Seja simp√°tica, use emojis com modera√ß√£o
8. Respostas curtas e diretas (ideal para WhatsApp)`;

      const result = await model.generateContent(prompt);
      const responseText = result.response.text();

      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        // Fallback
        return {
          analysis: {
            sentiment: 'neutral',
            urgency: 'normal',
            intent: 'other',
            confidence: 0,
            reasoning: 'Falha ao processar',
            shouldEscalate: false,
          },
          response: 'Oi! Desculpa, tive um probleminha aqui. Pode repetir? üòä',
        };
      }

      const parsed = JSON.parse(jsonMatch[0]);
      return {
        analysis: parsed.analysis,
        response: parsed.response,
      };

    } catch (error) {
      this.logger.error(`Error in analyzeAndRespond: ${error.message}`);
      return {
        analysis: {
          sentiment: 'neutral',
          urgency: 'normal',
          intent: 'other',
          confidence: 0,
          reasoning: 'Erro na API',
          shouldEscalate: false,
        },
        response: 'Oi! Desculpa, estou com um probleminha t√©cnico. Pode tentar de novo? üòÖ',
      };
    }
  }

  /**
   * Formata hist√≥rico de conversa de forma leg√≠vel para a IA
   */
  private formatConversationHistory(messages: any[], limit: number = 10): string {
    if (!messages || messages.length === 0) {
      return 'Nenhuma mensagem anterior.';
    }

    const lastMessages = messages.slice(-limit);

    return lastMessages.map((m: any, index: number) => {
      const role = m.direction === 'incoming' ? 'CLIENTE' : 'VOC√ä (Sofia)';
      const timestamp = m.createdAt
        ? new Date(m.createdAt).toLocaleString('pt-BR', {
          day: '2-digit',
          month: '2-digit',
          hour: '2-digit',
          minute: '2-digit'
        })
        : '';

      return `[${index + 1}] ${role}${timestamp ? ` (${timestamp})` : ''}: ${m.content}`;
    }).join('\n');
  }

  async parseProductConfirmation(
    messageContent: string,
    pendingProduct: any,
  ): Promise<{
    confirmed: boolean;
    productData?: {
      name: string;
      description?: string;
      price: number;
      quantity: number;
      variant?: string;
    };
    needsMoreInfo?: string;
  }> {
    try {
      const model = this.genAI.getGenerativeModel({
        model: this.MODEL_NAME,
        generationConfig: {
          temperature: 0.2,
          maxOutputTokens: 512,
        }
      });

      const prompt = `O usu√°rio est√° confirmando o cadastro de um produto.
      
      Produto pendente:
        - Nome: ${pendingProduct.name}
        - Descri√ß√£o: ${pendingProduct.description || 'N/A'}
        - Quantidade sugerida: ${pendingProduct.quantity || 'n√£o informada'}
      
      Mensagem do usu√°rio: "${messageContent}"
      
      Extraia as informa√ß√µes da mensagem:
        1. Se ele confirmou(disse sim, ok, confirma, etc)
        2. O pre√ßo mencionado(n√∫mero decimal, ex: 29.90)
        3. Quantidade(se mencionou)
        4. Variante / tamanho(se mencionou)
      
      Responda em JSON:
        {
          "confirmed": true / false,
            "price": 29.90(n√∫mero ou null),
              "quantity": 10(n√∫mero ou null),
                "variant": "Tamanho M"(string ou null),
                  "needsMoreInfo": "O que falta"(string ou null se tudo ok)
        }`; // JSON structure simplified for response

      // Fixing the prompt JSON structure string for actual usage
      const promptClean = `O usu√°rio est√° confirmando o cadastro de um produto.
      Produto pendente:
      - Nome: ${pendingProduct.name}
      - Descri√ß√£o: ${pendingProduct.description || 'N/A'}
      - Quantidade sugerida: ${pendingProduct.quantity || 'n√£o informada'}
      
      Mensagem do usu√°rio: "${messageContent}"
      
      Extraia as informa√ß√µes da mensagem e confirme se ele quer cadastrar.O pre√ßo √© obrigat√≥rio.
      
      Responda APENAS JSON:
      {
        "confirmed": boolean,
        "price": number | null,
        "quantity": number | null,
        "variant": string | null,
        "needsMoreInfo": string | null
      }`;

      const result = await model.generateContent(promptClean);
      const jsonMatch = result.response.text().match(/\{[\s\S]*\}/);

      if (!jsonMatch) {
        return { confirmed: false, needsMoreInfo: 'N√£o entendi. Pode confirmar o pre√ßo?' };
      }
      return JSON.parse(jsonMatch[0]);

    } catch (error) {
      this.logger.error(`Error parsing confirmation: ${error.message}`);
      return {
        confirmed: false, needsMoreInfo: 'Erro ao processar. Tente novamente.'
      };
    }
  }

  async parseOwnerCommand(messageContent: string, companyId: string): Promise<{
    isCommand: boolean;
    commandType?: 'instruction' | 'query' | 'config' | 'conversation';
    response?: string;
  }> {
    try {
      const model = this.genAI.getGenerativeModel({
        model: this.MODEL_NAME,
        generationConfig: {
          temperature: 0.3,
          maxOutputTokens: 512,
        }
      });

      const prompt = `Voc√™ √© uma secret√°ria virtual inteligente. Analise a mensagem do seu chefe(dono) e determine:
      
      1. Se √© um COMANDO / INSTRU√á√ÉO para voc√™(ex: "avise todos os clientes que...", "quando algu√©m perguntar sobre X, diga Y", "mude sua forma de responder")
      2. Se √© uma CONSULTA sobre algo(ex: "quantas mensagens hoje?", "quem me procurou?", "resumo do dia")
      3. Se √© apenas uma CONVERSA normal
      
      Mensagem do chefe: "${messageContent}"
      
      Responda em JSON:
      {
        "isCommand": true / false,
          "commandType": "instruction" | "query" | "conversation",
            "response": "Sua resposta ao chefe (confirme o comando ou responda a consulta)"
      }
      
      Se for instru√ß√£o, confirme que entendeu e vai seguir.
      Se for consulta, responda de forma √∫til.
      Se for conversa, responda naturalmente como assistente pessoal.`;

      const result = await model.generateContent(prompt);
      const response = result.response.text();

      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        // Mapear 'conversation' para undefined no commandType se n√£o for commando de verdade
        // O tipo esperado √© 'instruction' | 'query' | 'config'
        let type: any = parsed.commandType;
        if (type === 'conversation') {
          // Se for conversa, isCommand deve ser false?
          // O original dizia isCommand: boolean.
          // Se o prompt diz isCommand: true para queries tamb√©m?
          // O prompt original diz "Se √© COMANDO/INSTRU√á√ÉO". Queries s√£o consultas.
          // Vamos respeitar o retorno do modelo.
          if (type === 'conversation') type = undefined;
        }

        return {
          isCommand: parsed.isCommand,
          commandType: type,
          response: parsed.response
        };
      }

      return { isCommand: false, response: 'N√£o entendi.' };
    } catch (error) {
      this.logger.error(`Error parsing owner command: ${error.message}`);
      return { isCommand: false };
    }
  }
}

